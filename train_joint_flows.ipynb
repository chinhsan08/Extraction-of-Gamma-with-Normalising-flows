{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training of Three Normalizing Flows with Constraint\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **joint training** of all three normalizing flows:\n",
    "- `flow_flavor`: Models the D → Ksππ flavor-tagged amplitude\n",
    "- `flow_even`: Models the CP-even combination\n",
    "- `flow_odd`: Models the CP-odd combination\n",
    "\n",
    "Unlike the constrained training in `train_flow_odd_constrained.ipynb` where only `flow_odd` is trained,\n",
    "here all three flows are trained simultaneously with:\n",
    "1. Individual NLL losses for each flow\n",
    "2. A shared constraint penalty enforcing $|C| \\leq |A||\\bar{A}|$\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{-\\langle \\log p_{\\text{flavor}}(x_{\\text{flavor}}) \\rangle}_{\\text{NLL flavor}} + \\underbrace{-\\langle \\log p_{\\text{even}}(x_{\\text{even}}) \\rangle}_{\\text{NLL even}} + \\underbrace{-\\langle \\log p_{\\text{odd}}(x_{\\text{odd}}) \\rangle}_{\\text{NLL odd}} + \\lambda \\underbrace{\\langle \\max(|C| - \\text{abJ}, 0)^2 \\rangle}_{\\text{constraint}}$$\n",
    "\n",
    "where:\n",
    "- $C = \\frac{\\Gamma_+ \\cdot p_{\\text{even}} - \\Gamma_- \\cdot p_{\\text{odd}}}{\\Gamma_+ + \\Gamma_-}$\n",
    "- $\\text{abJ} = |A(s_{12}, s_{13})| \\cdot |A(s_{13}, s_{12})| = \\sqrt{p_{\\text{flavor}}(m', \\theta')} \\cdot \\sqrt{p_{\\text{flavor}}(m'_{\\text{swap}}, \\theta'_{\\text{swap}})}$\n",
    "\n",
    "The swapped SDP coordinates $(m'_{\\text{swap}}, \\theta'_{\\text{swap}})$ are computed via the standard\n",
    "`sdp_to_dp → swap → dp_to_sdp` transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nflows.flows import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms import CompositeTransform, RandomPermutation\n",
    "from nflows.transforms.coupling import PiecewiseRationalQuadraticCouplingTransform\n",
    "from nflows.transforms.base import Transform\n",
    "from nflows.transforms import Sigmoid, InverseTransform\n",
    "\n",
    "# Plotting setup\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Amplitude import DKpp, BKpp, DalitzSample, AmpSample, SquareDalitzPlot2\n",
    "\n",
    "# --- Particle masses ---\n",
    "mD, mKs, mpi = 1.86483, 0.497611, 0.13957018\n",
    "SDP = SquareDalitzPlot2(mD, mKs, mpi, mpi)\n",
    "\n",
    "# Initialize amplitude model\n",
    "dkpp = DKpp()\n",
    "sdp_obj = SquareDalitzPlot2(dkpp.M(), dkpp.m1(), dkpp.m2(), dkpp.m3())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Coordinate Transformation Functions\n",
    "\n",
    "Functions to convert between Square Dalitz Plot (SDP) and standard Dalitz Plot (DP) coordinates,\n",
    "and to compute swapped coordinates for the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdp_to_dp(points_sdp, sdp_obj, idx=(1,2,3)):\n",
    "    \"\"\"\n",
    "    Convert Square Dalitz Plot coordinates to Dalitz Plot coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points_sdp : ndarray, shape (N, 2)\n",
    "        Points in SDP coordinates (m', θ')\n",
    "    sdp_obj : SquareDalitzPlot2\n",
    "        SDP object with mass definitions\n",
    "    idx : tuple\n",
    "        Particle indices (i, j, k)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (N, 2)\n",
    "        Points in DP coordinates (s_ij, s_ik)\n",
    "    \"\"\"\n",
    "    i, j, k = idx\n",
    "    out = np.empty_like(points_sdp, dtype=float)\n",
    "    for n, (mp, th) in enumerate(points_sdp):\n",
    "        sij, sik = sdp_obj.M_from_MpT(mp, th, i, j, k)\n",
    "        out[n, 0] = sij\n",
    "        out[n, 1] = sik\n",
    "    return out\n",
    "\n",
    "\n",
    "def dp_to_sdp(points_dp, sdp_obj, idx=(1,2,3)):\n",
    "    \"\"\"\n",
    "    Convert Dalitz Plot coordinates to Square Dalitz Plot coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points_dp : ndarray, shape (N, 2)\n",
    "        Points in DP coordinates (s_ij, s_ik)\n",
    "    sdp_obj : SquareDalitzPlot2\n",
    "        SDP object with mass definitions\n",
    "    idx : tuple\n",
    "        Particle indices (i, j, k)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (N, 2)\n",
    "        Points in SDP coordinates (m', θ')\n",
    "    \"\"\"\n",
    "    i, j, k = idx\n",
    "    s12 = points_dp[:, 0]\n",
    "    s13 = points_dp[:, 1]\n",
    "    mp = np.vectorize(lambda a, b: sdp_obj.MpfromM(a, b, i, j, k), otypes=[float])(s12, s13)\n",
    "    tp = np.vectorize(lambda a, b: sdp_obj.TfromM(a, b, i, j, k), otypes=[float])(s12, s13)\n",
    "    return np.column_stack([mp, tp])\n",
    "\n",
    "\n",
    "def swap_to_other_pair_sdp(s12, s13, sdp_obj, pair_swap=(1,3,2)):\n",
    "    \"\"\"\n",
    "    Convert Dalitz point to SDP coordinates for a different particle pairing.\n",
    "    \n",
    "    This computes the SDP coordinates when swapping s12 <-> s13,\n",
    "    which corresponds to evaluating A(s13, s12) instead of A(s12, s13).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s12, s13 : ndarray\n",
    "        Dalitz plot coordinates\n",
    "    sdp_obj : SquareDalitzPlot2\n",
    "        SDP object with mass definitions\n",
    "    pair_swap : tuple\n",
    "        New particle indices after swap\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (N, 2)\n",
    "        Swapped SDP coordinates (m'_swap, θ'_swap)\n",
    "    \"\"\"\n",
    "    i2, j2, k2 = pair_swap\n",
    "    s12 = np.asarray(s12)\n",
    "    s13 = np.asarray(s13)\n",
    "    mp13 = np.empty_like(s12)\n",
    "    th13 = np.empty_like(s12)\n",
    "    for n in range(s12.size):\n",
    "        mp13[n] = sdp_obj.MpfromM(s13[n], s12[n], i2, j2, k2)\n",
    "        th13[n] = sdp_obj.TfromM(s13[n], s12[n], i2, j2, k2)\n",
    "    return np.column_stack([mp13, th13])\n",
    "\n",
    "\n",
    "def compute_swapped_sdp_coords(points_sdp, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2)):\n",
    "    \"\"\"\n",
    "    Compute swapped SDP coordinates for a batch of points.\n",
    "    \n",
    "    Pipeline: SDP -> DP -> swap -> SDP\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points_sdp : ndarray, shape (N, 2)\n",
    "        Original SDP coordinates (m', θ')\n",
    "    sdp_obj : SquareDalitzPlot2\n",
    "        SDP object\n",
    "    idx : tuple\n",
    "        Original particle indices\n",
    "    pair_swap : tuple\n",
    "        Swapped particle indices\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (N, 2)\n",
    "        Swapped SDP coordinates\n",
    "    \"\"\"\n",
    "    # SDP -> DP\n",
    "    dp = sdp_to_dp(points_sdp, sdp_obj, idx=idx)\n",
    "    s12, s13 = dp[:, 0], dp[:, 1]\n",
    "    \n",
    "    # Swap and convert back to SDP\n",
    "    swapped_sdp = swap_to_other_pair_sdp(s12, s13, sdp_obj, pair_swap=pair_swap)\n",
    "    \n",
    "    return swapped_sdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Architecture\n",
    "\n",
    "Neural spline flow architecture (same as in other notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for conditioning in coupling layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 hidden=64, layers=2, output_scale=0.30):\n",
    "        super().__init__()\n",
    "        feats = [nn.Linear(in_features, hidden), nn.SiLU()]\n",
    "        for _ in range(layers - 1):\n",
    "            feats += [nn.Linear(hidden, hidden), nn.SiLU()]\n",
    "        self.backbone = nn.Sequential(*feats)\n",
    "        self.head = nn.Linear(hidden, out_features)\n",
    "\n",
    "        nn.init.zeros_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        h = self.backbone(x)\n",
    "        return self.head(h) * self.output_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow(on_unit_box=True, num_flows=8, hidden_features=64, num_bins=8, device=None):\n",
    "    \"\"\"\n",
    "    Create a normalizing flow model using Neural Spline Flows.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    dim = 2\n",
    "    transforms = []\n",
    "\n",
    "    if on_unit_box:\n",
    "        sigmoid = Sigmoid()\n",
    "        if hasattr(sigmoid, 'temperature') and isinstance(sigmoid.temperature, torch.Tensor):\n",
    "            sigmoid.temperature = sigmoid.temperature.to(device)\n",
    "        if hasattr(sigmoid, 'eps') and isinstance(sigmoid.eps, torch.Tensor):\n",
    "            sigmoid.eps = sigmoid.eps.to(device)\n",
    "        transforms.append(InverseTransform(sigmoid))\n",
    "\n",
    "    masks = [torch.tensor([1, 0], dtype=torch.bool),\n",
    "             torch.tensor([0, 1], dtype=torch.bool)]\n",
    "\n",
    "    for i in range(num_flows):\n",
    "        mask = masks[i % 2]\n",
    "\n",
    "        def conditioner(in_features, out_features, _hidden=hidden_features):\n",
    "            return MLP(in_features, out_features, hidden=_hidden, layers=2)\n",
    "\n",
    "        transforms.append(\n",
    "            PiecewiseRationalQuadraticCouplingTransform(\n",
    "                mask=mask,\n",
    "                transform_net_create_fn=conditioner,\n",
    "                num_bins=num_bins,\n",
    "                tails=\"linear\",\n",
    "                tail_bound=5.0,\n",
    "                apply_unconditional_transform=False,\n",
    "            )\n",
    "        )\n",
    "        transforms.append(RandomPermutation(features=dim))\n",
    "\n",
    "    transform = CompositeTransform(transforms)\n",
    "    base = StandardNormal(shape=[dim])\n",
    "\n",
    "    return Flow(transform, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dataset Classes\n",
    "\n",
    "We need datasets that provide:\n",
    "1. Batches from all three data sources simultaneously\n",
    "2. Precomputed swapped coordinates for the constraint penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DalitzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Dataset for Dalitz plot coordinates.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDalitzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that provides synchronized batches from three data sources,\n",
    "    plus precomputed swapped coordinates for constraint evaluation.\n",
    "    \n",
    "    Each __getitem__ returns:\n",
    "        (x_flavor, x_even, x_odd, x_odd_swapped)\n",
    "    \n",
    "    where x_odd_swapped are the SDP coordinates after swapping s12 <-> s13.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_flavor, data_even, data_odd, data_odd_swapped):\n",
    "        self.data_flavor = torch.FloatTensor(data_flavor) if isinstance(data_flavor, np.ndarray) else data_flavor\n",
    "        self.data_even = torch.FloatTensor(data_even) if isinstance(data_even, np.ndarray) else data_even\n",
    "        self.data_odd = torch.FloatTensor(data_odd) if isinstance(data_odd, np.ndarray) else data_odd\n",
    "        self.data_odd_swapped = torch.FloatTensor(data_odd_swapped) if isinstance(data_odd_swapped, np.ndarray) else data_odd_swapped\n",
    "        \n",
    "        # Use minimum size for epoch length\n",
    "        self.epoch_len = min(len(self.data_flavor), len(self.data_even), len(self.data_odd))\n",
    "        \n",
    "        # Create indices for each dataset\n",
    "        self._shuffle_indices()\n",
    "    \n",
    "    def _shuffle_indices(self):\n",
    "        \"\"\"Shuffle indices for each dataset.\"\"\"\n",
    "        self.idx_flavor = torch.randperm(len(self.data_flavor))\n",
    "        self.idx_even = torch.randperm(len(self.data_even))\n",
    "        # odd and odd_swapped share the same indices (they're paired)\n",
    "        self.idx_odd = torch.randperm(len(self.data_odd))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.epoch_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Wrap indices for datasets larger than epoch_len\n",
    "        idx_f = self.idx_flavor[idx % len(self.data_flavor)]\n",
    "        idx_e = self.idx_even[idx % len(self.data_even)]\n",
    "        idx_o = self.idx_odd[idx % len(self.data_odd)]\n",
    "        \n",
    "        return (self.data_flavor[idx_f], \n",
    "                self.data_even[idx_e], \n",
    "                self.data_odd[idx_o],\n",
    "                self.data_odd_swapped[idx_o])  # Same index as odd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compute Gamma Values\n",
    "\n",
    "We need $\\Gamma_+$ and $\\Gamma_-$ for the constraint formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _finite_pos(x, eps=1e-14):\n",
    "    \"\"\"\n",
    "    Ensure array has finite positive values for numerical stability.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    x = np.where(np.isfinite(x), x, 0.0)\n",
    "    return np.maximum(x, eps)\n",
    "\n",
    "\n",
    "def compute_Gamma_pm_from_amplitude(dkpp_model, sdp_obj, idx=(1,2,3), nx=1000, ny=1000):\n",
    "    \"\"\"\n",
    "    Compute Gamma_plus and Gamma_minus from the amplitude model using numerical integration.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Gamma_plus, Gamma_minus : float\n",
    "    \"\"\"\n",
    "    u = np.linspace(0, 1, nx)\n",
    "    v = np.linspace(0, 1, ny)\n",
    "    U, V = np.meshgrid(u, v, indexing='xy')\n",
    "    pts_sdp = np.column_stack([U.ravel(), V.ravel()])\n",
    "\n",
    "    # Convert to Dalitz coordinates\n",
    "    S = sdp_to_dp(pts_sdp, sdp_obj, idx=idx)\n",
    "    s12 = S[:, 0]\n",
    "    s13 = S[:, 1]\n",
    "\n",
    "    # Compute amplitudes\n",
    "    A12  = dkpp_model.full(np.column_stack([s12, s13]))\n",
    "    A13s = dkpp_model.full(np.column_stack([s13, s12]))\n",
    "\n",
    "    A_plus  = A12 + A13s  # CP-even combination\n",
    "    A_minus = A12 - A13s  # CP-odd combination\n",
    "\n",
    "    # Compute Jacobian for integration measure\n",
    "    invJ = np.array([1.0 / sdp_obj.jacobian(s12[n], s13[n], *idx) \n",
    "                     for n in range(len(s12))])\n",
    "    invJ = np.where(np.isfinite(invJ), invJ, 0.0)\n",
    "\n",
    "    I_plus  = np.abs(A_plus)**2  * invJ\n",
    "    I_minus = np.abs(A_minus)**2 * invJ\n",
    "\n",
    "    du = 1.0/(nx-1)\n",
    "    dv = 1.0/(ny-1)\n",
    "    dA = du * dv\n",
    "\n",
    "    Gamma_plus  = np.sum(I_plus)  * dA\n",
    "    Gamma_minus = np.sum(I_minus) * dA\n",
    "\n",
    "    return Gamma_plus, Gamma_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Gamma_plus and Gamma_minus from amplitude model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/san/Library/CloudStorage/OneDrive-CornellUniversity/Extraction-of-Gamma-with-Normalising-flows/Amplitude.py:487: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  dmp_dsij = -(1.0 / np.pi) * (1.0 / np.sqrt(one_minus_x2)) * (1.0 / (denom_m * np.sqrt(mijSq)))\n",
      "/Users/san/Library/CloudStorage/OneDrive-CornellUniversity/Extraction-of-Gamma-with-Normalising-flows/Amplitude.py:426: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  cosHel = -(mikSq - self.mSq[i] - self.mSq[k] - 2.0 * EiCmsij * EkCmsij) / (2.0 * self.qi * self.qk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma_plus  = 949.7953\n",
      "Gamma_minus = 2008.2305\n",
      "Ratio Gamma_+/Gamma_- = 0.4730\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing Gamma_plus and Gamma_minus from amplitude model...\")\n",
    "gamma_p, gamma_m = compute_Gamma_pm_from_amplitude(dkpp, sdp_obj, idx=(1,2,3), nx=500, ny=500)\n",
    "print(f\"Gamma_plus  = {gamma_p:.4f}\")\n",
    "print(f\"Gamma_minus = {gamma_m:.4f}\")\n",
    "print(f\"Ratio Gamma_+/Gamma_- = {gamma_p/gamma_m:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Joint Training Function\n",
    "\n",
    "The core training loop that:\n",
    "1. Computes NLL for each flow using its own data (flavor, even, odd)\n",
    "2. Computes the constraint penalty on **B-decay points** (not random points)\n",
    "3. Uses precomputed swapped coordinates for abJ calculation\n",
    "\n",
    "**Key insight**: The constraint should be enforced on points that matter for B→DK analysis,\n",
    "not on uniformly sampled points. We use actual B-decay MC/data points for constraint evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint_flows_with_B_constraint(\n",
    "    flow_flavor,\n",
    "    flow_even,\n",
    "    flow_odd,\n",
    "    train_loader,\n",
    "    constraint_points,\n",
    "    constraint_points_swapped,\n",
    "    Gamma_plus=1.0,\n",
    "    Gamma_minus=1.0,\n",
    "    lam=10.0,\n",
    "    warmup_epochs=20,\n",
    "    num_epochs=200,\n",
    "    lr_odd=1e-3,\n",
    "    lr_even=1e-4,\n",
    "    lr_flavor=1e-4,\n",
    "    constraint_batch_size=10000,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train all three flows jointly with constraint evaluated on B-decay points.\n",
    "    \n",
    "    Loss = NLL_flavor + NLL_even + NLL_odd + λ * constraint_penalty\n",
    "    \n",
    "    The constraint penalty is evaluated on B-decay points (Bp, Bm, or MC),\n",
    "    ensuring the constraint |C| <= abJ is satisfied where it matters most.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    flow_flavor, flow_even, flow_odd : Flow\n",
    "        The three normalizing flow models.\n",
    "    train_loader : DataLoader\n",
    "        Must yield (x_flavor, x_even, x_odd, x_odd_swapped) tuples for NLL computation.\n",
    "    constraint_points : torch.Tensor, shape (N, 2)\n",
    "        B-decay points (SDP coordinates) where constraint should be enforced.\n",
    "        Can be Bp + Bm data, or MC points, or both combined.\n",
    "    constraint_points_swapped : torch.Tensor, shape (N, 2)\n",
    "        Swapped SDP coordinates for constraint_points.\n",
    "    Gamma_plus, Gamma_minus : float\n",
    "        CP-even/odd decay widths.\n",
    "    lam : float\n",
    "        Maximum penalty strength.\n",
    "    warmup_epochs : int\n",
    "        Number of epochs to linearly ramp lambda from 0 to lam.\n",
    "    num_epochs : int\n",
    "        Total training epochs.\n",
    "    lr_odd, lr_even, lr_flavor : float\n",
    "        Learning rates for each flow.\n",
    "    constraint_batch_size : int\n",
    "        Batch size for constraint evaluation (to avoid memory issues).\n",
    "    device : str\n",
    "        Torch device.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    flows : tuple (flow_flavor, flow_even, flow_odd)\n",
    "    history : dict\n",
    "    \"\"\"\n",
    "    flow_flavor.to(device)\n",
    "    flow_even.to(device)\n",
    "    flow_odd.to(device)\n",
    "    \n",
    "    # Move constraint points to device\n",
    "    constraint_points = constraint_points.to(device)\n",
    "    constraint_points_swapped = constraint_points_swapped.to(device)\n",
    "    n_constraint = len(constraint_points)\n",
    "    \n",
    "    # Separate parameter groups with different learning rates\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': flow_odd.parameters(), 'lr': lr_odd},\n",
    "        {'params': flow_even.parameters(), 'lr': lr_even},\n",
    "        {'params': flow_flavor.parameters(), 'lr': lr_flavor},\n",
    "    ])\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.65, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    denom = Gamma_minus + Gamma_plus\n",
    "    \n",
    "    history = {\n",
    "        'total_loss': [],\n",
    "        'nll_flavor': [],\n",
    "        'nll_even': [],\n",
    "        'nll_odd': [],\n",
    "        'penalty_loss': [],\n",
    "        'violation_frac': [],\n",
    "        'learning_rate_odd': [],\n",
    "        'learning_rate_even': [],\n",
    "        'learning_rate_flavor': [],\n",
    "        'lambda': [],\n",
    "        'epochs_trained': num_epochs\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training\", ncols=80):\n",
    "        flow_flavor.train()\n",
    "        flow_even.train()\n",
    "        flow_odd.train()\n",
    "        \n",
    "        total_nll_flavor = 0.0\n",
    "        total_nll_even = 0.0\n",
    "        total_nll_odd = 0.0\n",
    "        total_pts = 0\n",
    "        \n",
    "        # Linear warmup: ramp lambda from 0 to lam over warmup_epochs\n",
    "        current_lam = lam * min(epoch / warmup_epochs, 1.0)\n",
    "        \n",
    "        # ===== Part 1: NLL training on flavor/even/odd data =====\n",
    "        for x_flavor, x_even, x_odd, _ in train_loader:\n",
    "            x_flavor = x_flavor.to(device)\n",
    "            x_even = x_even.to(device)\n",
    "            x_odd = x_odd.to(device)\n",
    "            B = x_odd.size(0)\n",
    "            \n",
    "            # NLL terms (each flow gets its own data)\n",
    "            nll_flavor = -flow_flavor.log_prob(x_flavor).mean()\n",
    "            nll_even = -flow_even.log_prob(x_even).mean()\n",
    "            nll_odd = -flow_odd.log_prob(x_odd).mean()\n",
    "            \n",
    "            # NLL-only backward (no constraint yet)\n",
    "            nll_loss = nll_flavor + nll_even + nll_odd\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            nll_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_nll_flavor += nll_flavor.item() * B\n",
    "            total_nll_even += nll_even.item() * B\n",
    "            total_nll_odd += nll_odd.item() * B\n",
    "            total_pts += B\n",
    "        \n",
    "        # ===== Part 2: Constraint penalty on B-decay points =====\n",
    "        total_pen = 0.0\n",
    "        total_viol = 0\n",
    "        \n",
    "        # Process constraint points in batches\n",
    "        for start_idx in range(0, n_constraint, constraint_batch_size):\n",
    "            end_idx = min(start_idx + constraint_batch_size, n_constraint)\n",
    "            x_B = constraint_points[start_idx:end_idx]\n",
    "            x_B_swapped = constraint_points_swapped[start_idx:end_idx]\n",
    "            B_c = len(x_B)\n",
    "            \n",
    "            # Evaluate flows at B-decay points\n",
    "            log_p_even = flow_even.log_prob(x_B)\n",
    "            log_p_odd = flow_odd.log_prob(x_B)\n",
    "            p_even = torch.exp(log_p_even)\n",
    "            p_odd = torch.exp(log_p_odd)\n",
    "            \n",
    "            # abJ = sqrt(p_flavor(x)) * sqrt(p_flavor(x_swapped))\n",
    "            log_p_flavor_orig = flow_flavor.log_prob(x_B)\n",
    "            log_p_flavor_swap = flow_flavor.log_prob(x_B_swapped)\n",
    "            abJ = torch.exp(0.5 * (log_p_flavor_orig + log_p_flavor_swap))\n",
    "            \n",
    "            # C = (Γ+ * p_even - Γ- * p_odd) / (Γ+ + Γ-)\n",
    "            C = (Gamma_plus * p_even - Gamma_minus * p_odd) / denom\n",
    "            \n",
    "            # Penalty: max(|C| - abJ, 0)^2\n",
    "            violation = torch.clamp(torch.abs(C) - abJ, min=0.0)\n",
    "            penalty = (violation ** 1).mean()\n",
    "            \n",
    "            # Constraint backward\n",
    "            constraint_loss = current_lam * penalty\n",
    "            optimizer.zero_grad()\n",
    "            constraint_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_pen += penalty.item() * B_c\n",
    "            total_viol += (violation > 0).sum().item()\n",
    "        \n",
    "        # Epoch averages\n",
    "        avg_nll_flavor = total_nll_flavor / total_pts\n",
    "        avg_nll_even = total_nll_even / total_pts\n",
    "        avg_nll_odd = total_nll_odd / total_pts\n",
    "        avg_pen = total_pen / n_constraint\n",
    "        viol_frac = total_viol / n_constraint\n",
    "        \n",
    "        total_loss = avg_nll_flavor + avg_nll_even + avg_nll_odd + current_lam * avg_pen\n",
    "        \n",
    "        history['total_loss'].append(total_loss)\n",
    "        history['nll_flavor'].append(avg_nll_flavor)\n",
    "        history['nll_even'].append(avg_nll_even)\n",
    "        history['nll_odd'].append(avg_nll_odd)\n",
    "        history['penalty_loss'].append(avg_pen)\n",
    "        history['violation_frac'].append(viol_frac)\n",
    "        history['learning_rate_odd'].append(optimizer.param_groups[0]['lr'])\n",
    "        history['learning_rate_even'].append(optimizer.param_groups[1]['lr'])\n",
    "        history['learning_rate_flavor'].append(optimizer.param_groups[2]['lr'])\n",
    "        history['lambda'].append(current_lam)\n",
    "        \n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"[{epoch:03d}] NLL: flav={avg_nll_flavor:.4f} even={avg_nll_even:.4f} \"\n",
    "                  f\"odd={avg_nll_odd:.4f} | Pen={avg_pen:.2e} Viol={viol_frac:.4f} λ={current_lam:.1f}\")\n",
    "    \n",
    "    return (flow_flavor, flow_even, flow_odd), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Load Data and Precompute Swapped Coordinates\n",
    "\n",
    "**Choose ONE of the following cells to run:**\n",
    "- **Cell 7a (Test)**: Uses small test datasets (100k events) for quick testing\n",
    "- **Cell 7b (Production)**: Uses full datasets with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST CONFIGURATION ===\n",
      "Loading test datasets (100k events each)...\n",
      "  Flavor: 100,000 events\n",
      "  Even:   100,000 events\n",
      "  Odd:    100,000 events\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7a. TEST CONFIGURATION - Small datasets for quick testing\n",
    "# =============================================================================\n",
    "# Run this cell for testing on local machine without full data\n",
    "\n",
    "data_flavor_path = \"D_Kspipi_SDP_test.npy\"       # 100k flavor events\n",
    "data_even_path = \"D_Kspipi_even_SDP_test.npy\"    # 100k CP-even events\n",
    "data_odd_path = \"D_Kspipi_odd_SDP_test.npy\"      # 100k CP-odd events\n",
    "\n",
    "print(\"=== TEST CONFIGURATION ===\")\n",
    "print(\"Loading test datasets (100k events each)...\")\n",
    "data_flavor = np.load(data_flavor_path)\n",
    "data_even = np.load(data_even_path)\n",
    "data_odd = np.load(data_odd_path)\n",
    "\n",
    "# Use all test data (no subsampling)\n",
    "train_flavor = data_flavor\n",
    "train_even = data_even\n",
    "train_odd = data_odd\n",
    "\n",
    "print(f\"  Flavor: {len(train_flavor):,} events\")\n",
    "print(f\"  Even:   {len(train_even):,} events\")\n",
    "print(f\"  Odd:    {len(train_odd):,} events\")\n",
    "\n",
    "# Test configuration flags\n",
    "USE_PRETRAINED = False\n",
    "BATCH_SIZE = 10000\n",
    "FLOW_CONFIG = {'num_flows': 16, 'hidden_features': 64, 'num_bins': 16}\n",
    "TRAIN_CONFIG_OVERRIDE = {'num_epochs': 100, 'warmup_epochs': 10, 'lr_odd': 1e-3, 'lr_even': 1e-3, 'lr_flavor': 1e-3}\n",
    "N_TEST_POINTS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7b. PRODUCTION CONFIGURATION - Full datasets with pretrained weights\n",
    "# =============================================================================\n",
    "# Run this cell on the other computer with full data and pretrained models\n",
    "# Skip cell 7a above if running this one\n",
    "\n",
    "data_flavor_path = \"D_Kspipi_SDP_2e7.npy\"        # ~20M flavor events\n",
    "data_even_path = \"D_Kspipi_even_SDP_2e7.npy\"     # ~20M CP-even events  \n",
    "data_odd_path = \"D_Kspipi_odd_SDP_2e7.npy\"       # ~20M CP-odd events\n",
    "\n",
    "print(\"=== PRODUCTION CONFIGURATION ===\")\n",
    "print(\"Loading full datasets...\")\n",
    "data_flavor = np.load(data_flavor_path)\n",
    "data_even = np.load(data_even_path)\n",
    "data_odd = np.load(data_odd_path)\n",
    "\n",
    "print(f\"  Flavor: {len(data_flavor):,} events\")\n",
    "print(f\"  Even:   {len(data_even):,} events\")\n",
    "print(f\"  Odd:    {len(data_odd):,} events\")\n",
    "\n",
    "# Subsample for training\n",
    "train_size = 2_000_000\n",
    "np.random.seed(42)\n",
    "idx_flavor = np.random.choice(len(data_flavor), size=min(train_size, len(data_flavor)), replace=False)\n",
    "idx_even = np.random.choice(len(data_even), size=min(train_size, len(data_even)), replace=False)\n",
    "idx_odd = np.random.choice(len(data_odd), size=min(train_size, len(data_odd)), replace=False)\n",
    "\n",
    "train_flavor = data_flavor[idx_flavor]\n",
    "train_even = data_even[idx_even]\n",
    "train_odd = data_odd[idx_odd]\n",
    "\n",
    "print(f\"\\nTraining with:\")\n",
    "print(f\"  Flavor: {len(train_flavor):,} events\")\n",
    "print(f\"  Even:   {len(train_even):,} events\")\n",
    "print(f\"  Odd:    {len(train_odd):,} events\")\n",
    "\n",
    "# Production configuration flags\n",
    "USE_PRETRAINED = True\n",
    "BATCH_SIZE = 50000\n",
    "FLOW_CONFIG = {'num_flows': 12, 'hidden_features': 128, 'num_bins': 24}\n",
    "TRAIN_CONFIG_OVERRIDE = {'num_epochs': 100, 'warmup_epochs': 10, 'lr_odd': 1e-3, 'lr_even': 1e-4, 'lr_flavor': 1e-4}\n",
    "N_TEST_POINTS = 200000\n",
    "\n",
    "# Pretrained model paths (adjust if needed)\n",
    "PRETRAINED_FLAVOR = 'test_ensemble_2e6/trial_seed1.pth'\n",
    "PRETRAINED_EVEN = 'test_ensemble_even_2e6/trial_seed1.pth'\n",
    "PRETRAINED_ODD = 'test_ensemble_odd_2e6/trial_seed1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with:\n",
      "  Flavor: 100,000 events\n",
      "  Even:   100,000 events\n",
      "  Odd:    100,000 events\n"
     ]
    }
   ],
   "source": [
    "# For test data, use all available events (no subsampling needed)\n",
    "# For larger datasets, uncomment the subsampling code below\n",
    "\n",
    "train_flavor = data_flavor\n",
    "train_even = data_even\n",
    "train_odd = data_odd\n",
    "\n",
    "# # Subsample for training (uncomment for large datasets)\n",
    "# train_size = 2_000_000\n",
    "# np.random.seed(42)\n",
    "# idx_flavor = np.random.choice(len(data_flavor), size=min(train_size, len(data_flavor)), replace=False)\n",
    "# idx_even = np.random.choice(len(data_even), size=min(train_size, len(data_even)), replace=False)\n",
    "# idx_odd = np.random.choice(len(data_odd), size=min(train_size, len(data_odd)), replace=False)\n",
    "# train_flavor = data_flavor[idx_flavor]\n",
    "# train_even = data_even[idx_even]\n",
    "# train_odd = data_odd[idx_odd]\n",
    "\n",
    "print(f\"Training with:\")\n",
    "print(f\"  Flavor: {len(train_flavor):,} events\")\n",
    "print(f\"  Even:   {len(train_even):,} events\")\n",
    "print(f\"  Odd:    {len(train_odd):,} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing swapped SDP coordinates for NLL training data...\n",
      "Done. Shape: (100000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Precompute swapped coordinates for train_odd (used for NLL data loader, not constraint)\n",
    "print(\"Precomputing swapped SDP coordinates for NLL training data...\")\n",
    "\n",
    "train_odd_swapped = compute_swapped_sdp_coords(\n",
    "    train_odd, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2)\n",
    ")\n",
    "\n",
    "print(f\"Done. Shape: {train_odd_swapped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7b. Load B-decay Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading B-decay data from BpBm_samples.npz...\n",
      "\n",
      "Loaded B-decay data:\n",
      "  B+ data (dataP_sdp):  50,000 events\n",
      "  B- data (dataM_sdp):  50,000 events\n",
      "  MC (mcM_sdp):         500,000 events\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load B-decay data for constraint evaluation\n",
    "# =============================================================================\n",
    "# The npz file structure (from Gamma-fit-pipeline):\n",
    "#   - 'dataP_sdp': B+ experimental data (~50k), shape (N, 2) in SDP coordinates\n",
    "#   - 'dataM_sdp': B- experimental data (~50k), shape (N, 2) in SDP coordinates  \n",
    "#   - 'mcM_sdp': B- Monte Carlo (~500k), shape (N, 2) in SDP coordinates\n",
    "#\n",
    "# We only need one MC sample (mcM_sdp) since it covers the same phase space\n",
    "\n",
    "B_DATA_PATH = \"BpBm_samples.npz\"\n",
    "\n",
    "print(f\"Loading B-decay data from {B_DATA_PATH}...\")\n",
    "B_data = np.load(B_DATA_PATH)\n",
    "\n",
    "# Load using the keys from Gamma-fit-pipeline\n",
    "Bp_sdp = B_data['dataP_sdp']  # B+ experimental data\n",
    "Bm_sdp = B_data['dataM_sdp']  # B- experimental data\n",
    "MC_sdp = B_data['mcM_sdp']    # Use B- MC only (mcP_sdp covers same phase space)\n",
    "\n",
    "print(f\"\\nLoaded B-decay data:\")\n",
    "print(f\"  B+ data (dataP_sdp):  {len(Bp_sdp):,} events\")\n",
    "print(f\"  B- data (dataM_sdp):  {len(Bm_sdp):,} events\")\n",
    "print(f\"  MC (mcM_sdp):         {len(MC_sdp):,} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ALL B-decay points for constraint: 600,000\n",
      "\n",
      "Precomputing swapped SDP coordinates for constraint points...\n",
      "(This may take a few minutes for large datasets)\n",
      "Done. Constraint points shape: torch.Size([600000, 2])\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Choose which B-decay points to use for constraint evaluation\n",
    "# =============================================================================\n",
    "# Options:\n",
    "#   1. Use all points (Bp + Bm + MC) - most comprehensive\n",
    "#   2. Use only data (Bp + Bm) - focuses on actual measurements\n",
    "#   3. Use only MC - if data is blinded or not available\n",
    "\n",
    "CONSTRAINT_MODE = \"all\"  # Options: \"all\", \"data_only\", \"mc_only\"\n",
    "\n",
    "if CONSTRAINT_MODE == \"all\":\n",
    "    constraint_points_np = np.vstack([Bp_sdp, Bm_sdp, MC_sdp])\n",
    "    print(f\"Using ALL B-decay points for constraint: {len(constraint_points_np):,}\")\n",
    "elif CONSTRAINT_MODE == \"data_only\":\n",
    "    constraint_points_np = np.vstack([Bp_sdp, Bm_sdp])\n",
    "    print(f\"Using DATA ONLY (Bp + Bm) for constraint: {len(constraint_points_np):,}\")\n",
    "elif CONSTRAINT_MODE == \"mc_only\":\n",
    "    constraint_points_np = MC_sdp\n",
    "    print(f\"Using MC ONLY for constraint: {len(constraint_points_np):,}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown CONSTRAINT_MODE: {CONSTRAINT_MODE}\")\n",
    "\n",
    "# Precompute swapped coordinates for constraint points\n",
    "print(\"\\nPrecomputing swapped SDP coordinates for constraint points...\")\n",
    "print(\"(This may take a few minutes for large datasets)\")\n",
    "\n",
    "constraint_points_swapped_np = compute_swapped_sdp_coords(\n",
    "    constraint_points_np, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2)\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "constraint_points = torch.from_numpy(constraint_points_np.astype(np.float32))\n",
    "constraint_points_swapped = torch.from_numpy(constraint_points_swapped_np.astype(np.float32))\n",
    "\n",
    "print(f\"Done. Constraint points shape: {constraint_points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10000\n",
      "Batches per epoch: 10\n"
     ]
    }
   ],
   "source": [
    "# Create joint dataset and loader\n",
    "# Uses BATCH_SIZE from configuration cell (7a or 7b)\n",
    "\n",
    "joint_dataset = JointDalitzDataset(train_flavor, train_even, train_odd, train_odd_swapped)\n",
    "joint_loader = DataLoader(\n",
    "    joint_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device == \"cuda\")\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Batches per epoch: {len(joint_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Initialize Flows from Pretrained Weights\n",
    "\n",
    "We start from pretrained flows to:\n",
    "1. Speed up convergence\n",
    "2. Preserve good individual NLL performance\n",
    "3. Focus training on satisfying the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow config: {'num_flows': 8, 'hidden_features': 64, 'num_bins': 16}\n",
      "Each flow has 58,744 parameters\n",
      "Total parameters: 176,232\n",
      "\n",
      "Training from scratch (no pretrained weights).\n"
     ]
    }
   ],
   "source": [
    "# Create flows using FLOW_CONFIG from configuration cell (7a or 7b)\n",
    "\n",
    "flow_flavor = create_flow(**FLOW_CONFIG)\n",
    "flow_even = create_flow(**FLOW_CONFIG)\n",
    "flow_odd = create_flow(**FLOW_CONFIG)\n",
    "\n",
    "n_params = sum(p.numel() for p in flow_flavor.parameters())\n",
    "print(f\"Flow config: {FLOW_CONFIG}\")\n",
    "print(f\"Each flow has {n_params:,} parameters\")\n",
    "print(f\"Total parameters: {3 * n_params:,}\")\n",
    "\n",
    "# Load pretrained weights if USE_PRETRAINED is True\n",
    "if USE_PRETRAINED:\n",
    "    print(f\"\\nLoading pretrained weights...\")\n",
    "    flow_flavor.load_state_dict(torch.load(PRETRAINED_FLAVOR, map_location=device))\n",
    "    flow_even.load_state_dict(torch.load(PRETRAINED_EVEN, map_location=device))\n",
    "    flow_odd.load_state_dict(torch.load(PRETRAINED_ODD, map_location=device))\n",
    "    print(f\"  Flavor: {PRETRAINED_FLAVOR}\")\n",
    "    print(f\"  Even:   {PRETRAINED_EVEN}\")\n",
    "    print(f\"  Odd:    {PRETRAINED_ODD}\")\n",
    "else:\n",
    "    print(\"\\nTraining from scratch (no pretrained weights).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Train Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Gamma_plus: 949.7952740443438\n",
      "  Gamma_minus: 2008.2304765864694\n",
      "  lam: 10.0\n",
      "  device: cpu\n",
      "  num_epochs: 100\n",
      "  warmup_epochs: 10\n",
      "  lr_odd: 0.001\n",
      "  lr_even: 0.001\n",
      "  lr_flavor: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# Uses TRAIN_CONFIG_OVERRIDE from configuration cell (7a or 7b)\n",
    "\n",
    "train_config = {\n",
    "    'Gamma_plus': gamma_p,\n",
    "    'Gamma_minus': gamma_m,\n",
    "    'lam': 10.0,\n",
    "    'device': device,\n",
    "    **TRAIN_CONFIG_OVERRIDE  # Override with test or production settings\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for k, v in train_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                              | 1/100 [00:53<1:28:26, 53.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] NLL: flav=-1.0827 even=-0.9246 odd=-0.8772 | Pen=3.12e-02 Viol=0.1751 λ=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▌                             | 5/100 [04:08<1:17:15, 48.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005] NLL: flav=-1.0784 even=-0.9134 odd=-0.8882 | Pen=7.28e-03 Viol=0.1025 λ=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███                           | 10/100 [08:24<1:16:32, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010] NLL: flav=-1.0708 even=-0.9332 odd=-0.8906 | Pen=1.61e-03 Viol=0.0470 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|████▏                         | 14/100 [11:43<1:12:04, 50.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run training with B-decay constraint points\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m (flow_flavor_trained, flow_even_trained, flow_odd_trained), history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_joint_flows_with_B_constraint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_flavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_even\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_odd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint_points_swapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Process constraint points in batches\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_config\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mtrain_joint_flows_with_B_constraint\u001b[0;34m(flow_flavor, flow_even, flow_odd, train_loader, constraint_points, constraint_points_swapped, Gamma_plus, Gamma_minus, lam, warmup_epochs, num_epochs, lr_odd, lr_even, lr_flavor, constraint_batch_size, device)\u001b[0m\n\u001b[1;32m    161\u001b[0m constraint_loss \u001b[38;5;241m=\u001b[39m current_lam \u001b[38;5;241m*\u001b[39m penalty\n\u001b[1;32m    162\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 163\u001b[0m \u001b[43mconstraint_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    166\u001b[0m total_pen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m penalty\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m B_c\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training with B-decay constraint points\n",
    "(flow_flavor_trained, flow_even_trained, flow_odd_trained), history = train_joint_flows_with_B_constraint(\n",
    "    flow_flavor,\n",
    "    flow_even,\n",
    "    flow_odd,\n",
    "    joint_loader,\n",
    "    constraint_points,\n",
    "    constraint_points_swapped,\n",
    "    constraint_batch_size=50000,  # Process constraint points in batches\n",
    "    **train_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                              | 1/100 [00:57<1:34:43, 57.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] NLL: flav=-1.0339 even=-0.8625 odd=-0.8347 | Pen=2.62e-02 Viol=0.2074 λ=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▌                             | 5/100 [05:25<1:47:48, 68.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005] NLL: flav=-1.0255 even=-0.8563 odd=-0.8084 | Pen=4.32e-03 Viol=0.1412 λ=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███                           | 10/100 [10:39<1:38:39, 65.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010] NLL: flav=-0.9197 even=-0.8523 odd=-0.8025 | Pen=8.52e-03 Viol=0.2051 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|████▌                         | 15/100 [15:18<1:22:13, 58.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015] NLL: flav=-1.0491 even=-0.8733 odd=-0.8458 | Pen=3.94e-04 Viol=0.1012 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████                        | 20/100 [19:55<1:15:04, 56.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[020] NLL: flav=-1.0580 even=-0.8925 odd=-0.8463 | Pen=4.50e-04 Viol=0.1023 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|███████▌                      | 25/100 [24:42<1:12:12, 57.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[025] NLL: flav=-1.0611 even=-0.9036 odd=-0.8835 | Pen=2.27e-04 Viol=0.1052 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|█████████                     | 30/100 [29:17<1:04:37, 55.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[030] NLL: flav=-1.0638 even=-0.9042 odd=-0.8753 | Pen=3.48e-04 Viol=0.1139 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███████████▏                    | 35/100 [33:39<56:47, 52.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[035] NLL: flav=-1.0757 even=-0.9040 odd=-0.8802 | Pen=2.72e-04 Viol=0.1110 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████████████▊                   | 40/100 [38:10<53:59, 53.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[040] NLL: flav=-1.0202 even=-0.9028 odd=-0.8281 | Pen=4.42e-04 Viol=0.1490 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|██████████████▍                 | 45/100 [42:56<51:07, 55.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[045] NLL: flav=-1.0686 even=-0.9213 odd=-0.8872 | Pen=8.49e-04 Viol=0.1450 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████████████████                | 50/100 [47:31<45:57, 55.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[050] NLL: flav=-1.0881 even=-0.9244 odd=-0.8945 | Pen=3.47e-04 Viol=0.1221 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████████████████▌              | 55/100 [52:06<41:18, 55.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[055] NLL: flav=-1.0942 even=-0.9319 odd=-0.9024 | Pen=2.14e-04 Viol=0.1032 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|███████████████████▏            | 60/100 [56:47<37:08, 55.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[060] NLL: flav=-1.0867 even=-0.9327 odd=-0.8979 | Pen=3.76e-04 Viol=0.1214 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|███████████████████▌          | 65/100 [1:01:21<32:10, 55.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[065] NLL: flav=-1.0973 even=-0.9369 odd=-0.9064 | Pen=2.14e-04 Viol=0.1073 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|█████████████████████         | 70/100 [1:06:00<27:34, 55.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[070] NLL: flav=-1.0980 even=-0.9389 odd=-0.9067 | Pen=1.68e-04 Viol=0.1012 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|██████████████████████▌       | 75/100 [1:10:29<22:38, 54.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[075] NLL: flav=-1.0972 even=-0.9385 odd=-0.9070 | Pen=2.31e-04 Viol=0.1058 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████████████████      | 80/100 [1:14:58<17:56, 53.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[080] NLL: flav=-1.0972 even=-0.9371 odd=-0.9041 | Pen=1.80e-04 Viol=0.1022 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|█████████████████████████▌    | 85/100 [1:19:27<13:37, 54.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[085] NLL: flav=-1.1011 even=-0.9431 odd=-0.9101 | Pen=1.57e-04 Viol=0.1036 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|███████████████████████████   | 90/100 [1:24:03<09:11, 55.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[090] NLL: flav=-1.1002 even=-0.9434 odd=-0.9096 | Pen=1.22e-04 Viol=0.0952 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|████████████████████████████▌ | 95/100 [1:28:27<04:26, 53.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[095] NLL: flav=-1.1022 even=-0.9450 odd=-0.9115 | Pen=1.61e-04 Viol=0.1033 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 100/100 [1:32:55<00:00, 55.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] NLL: flav=-1.0992 even=-0.9397 odd=-0.9096 | Pen=3.34e-04 Viol=0.1033 λ=10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training with B-decay constraint points\n",
    "(flow_flavor_trained, flow_even_trained, flow_odd_trained), history = train_joint_flows_with_B_constraint(\n",
    "    flow_flavor,\n",
    "    flow_even,\n",
    "    flow_odd,\n",
    "    joint_loader,\n",
    "    constraint_points,\n",
    "    constraint_points_swapped,\n",
    "    constraint_batch_size=50000,  # Process constraint points in batches\n",
    "    **train_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "output_dir = Path(\"joint_trained_flows\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(flow_flavor_trained.state_dict(), output_dir / \"flow_flavor.pth\")\n",
    "torch.save(flow_even_trained.state_dict(), output_dir / \"flow_even.pth\")\n",
    "torch.save(flow_odd_trained.state_dict(), output_dir / \"flow_odd.pth\")\n",
    "\n",
    "# Save training history\n",
    "with open(output_dir / \"history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    **train_config,\n",
    "    **flow_config,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir / \"config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Models saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# NLL losses\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['nll_flavor'], label='flavor', linewidth=2)\n",
    "ax.plot(history['nll_even'], label='even', linewidth=2)\n",
    "ax.plot(history['nll_odd'], label='odd', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('NLL')\n",
    "ax.set_title('Negative Log-Likelihood')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Total loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['total_loss'], linewidth=2, color='black')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Total Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Penalty loss\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history['penalty_loss'], linewidth=2, color='firebrick')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Penalty')\n",
    "ax.set_title('Constraint Penalty')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Violation fraction\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['violation_frac'], linewidth=2, color='darkorange')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Fraction')\n",
    "ax.set_title('Violation Fraction (|C| > abJ)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Lambda schedule\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['lambda'], linewidth=2, color='seagreen')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel(r'$\\lambda$')\n",
    "ax.set_title(r'$\\lambda$ Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rates\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history['learning_rate_odd'], label='odd', linewidth=2)\n",
    "ax.plot(history['learning_rate_even'], label='even', linewidth=2)\n",
    "ax.plot(history['learning_rate_flavor'], label='flavor', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rates')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history.pdf', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal NLLs:\")\n",
    "print(f\"  Flavor: {history['nll_flavor'][-1]:.6f}\")\n",
    "print(f\"  Even:   {history['nll_even'][-1]:.6f}\")\n",
    "print(f\"  Odd:    {history['nll_odd'][-1]:.6f}\")\n",
    "print(f\"\\nFinal violation fraction: {history['violation_frac'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Evaluate Constraint Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_violation_rate_on_points(flow_flavor, flow_even, flow_odd,\n",
    "                                    points, points_swapped,\n",
    "                                    Gamma_plus, Gamma_minus,\n",
    "                                    device='cpu', label=\"\"):\n",
    "    \"\"\"\n",
    "    Check what fraction of given points violate |C| > abJ.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points : ndarray or Tensor, shape (N, 2)\n",
    "        SDP coordinates to check\n",
    "    points_swapped : ndarray or Tensor, shape (N, 2)\n",
    "        Swapped SDP coordinates\n",
    "    \"\"\"\n",
    "    flow_flavor.eval()\n",
    "    flow_even.eval()\n",
    "    flow_odd.eval()\n",
    "    \n",
    "    if isinstance(points, np.ndarray):\n",
    "        pts_t = torch.from_numpy(points.astype(np.float32)).to(device)\n",
    "        pts_swapped_t = torch.from_numpy(points_swapped.astype(np.float32)).to(device)\n",
    "    else:\n",
    "        pts_t = points.to(device)\n",
    "        pts_swapped_t = points_swapped.to(device)\n",
    "    \n",
    "    n_test = len(pts_t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        p_even = torch.exp(flow_even.log_prob(pts_t)).cpu().numpy()\n",
    "        p_odd = torch.exp(flow_odd.log_prob(pts_t)).cpu().numpy()\n",
    "        \n",
    "        log_p_flavor_orig = flow_flavor.log_prob(pts_t)\n",
    "        log_p_flavor_swap = flow_flavor.log_prob(pts_swapped_t)\n",
    "        abJ = torch.exp(0.5 * (log_p_flavor_orig + log_p_flavor_swap)).cpu().numpy()\n",
    "    \n",
    "    denom = Gamma_plus + Gamma_minus\n",
    "    C = (Gamma_plus * p_even - Gamma_minus * p_odd) / denom\n",
    "    \n",
    "    violated = np.abs(C) > abJ\n",
    "    ratio = np.abs(C) / np.maximum(abJ, 1e-10)\n",
    "    \n",
    "    print(f\"  {label} ({n_test:,} points):\")\n",
    "    print(f\"    Violation rate: {violated.mean():.4f} ({violated.sum():,}/{n_test:,})\")\n",
    "    print(f\"    Max |C|/abJ: {ratio.max():.4f}\")\n",
    "    if violated.any():\n",
    "        print(f\"    Mean |C|/abJ (violated only): {ratio[violated].mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"    No violations!\")\n",
    "    \n",
    "    return C, abJ, violated, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate constraint violations on B-decay points\n",
    "print(\"=== CONSTRAINT VIOLATIONS ON B-DECAY POINTS ===\\n\")\n",
    "\n",
    "# Precompute swapped coordinates for each subset\n",
    "Bp_swapped = compute_swapped_sdp_coords(Bp_sdp, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2))\n",
    "Bm_swapped = compute_swapped_sdp_coords(Bm_sdp, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2))\n",
    "MC_swapped = compute_swapped_sdp_coords(MC_sdp, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2))\n",
    "\n",
    "# Check each subset\n",
    "C_Bp, abJ_Bp, viol_Bp, ratio_Bp = check_violation_rate_on_points(\n",
    "    flow_flavor_trained, flow_even_trained, flow_odd_trained,\n",
    "    Bp_sdp, Bp_swapped, gamma_p, gamma_m, device=device, label=\"B+ data\"\n",
    ")\n",
    "\n",
    "C_Bm, abJ_Bm, viol_Bm, ratio_Bm = check_violation_rate_on_points(\n",
    "    flow_flavor_trained, flow_even_trained, flow_odd_trained,\n",
    "    Bm_sdp, Bm_swapped, gamma_p, gamma_m, device=device, label=\"B- data\"\n",
    ")\n",
    "\n",
    "C_MC, abJ_MC, viol_MC, ratio_MC = check_violation_rate_on_points(\n",
    "    flow_flavor_trained, flow_even_trained, flow_odd_trained,\n",
    "    MC_sdp, MC_swapped, gamma_p, gamma_m, device=device, label=\"MC\"\n",
    ")\n",
    "\n",
    "# Combined summary\n",
    "all_viol = np.concatenate([viol_Bp, viol_Bm, viol_MC])\n",
    "print(f\"\\n  COMBINED: {all_viol.mean():.4f} violation rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize violations on B+ and B- data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, pts, viol, ratio, title in [\n",
    "    (axes[0], Bp_sdp, viol_Bp, ratio_Bp, \"B+ data\"),\n",
    "    (axes[1], Bm_sdp, viol_Bm, ratio_Bm, \"B- data\"),\n",
    "    (axes[2], MC_sdp[:50000], viol_MC[:50000], ratio_MC[:50000], \"MC (first 50k)\")  # Subsample MC for plotting\n",
    "]:\n",
    "    # Background: all points in light gray\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], c='lightgray', s=1, alpha=0.3, rasterized=True)\n",
    "    \n",
    "    # Violated points colored by severity\n",
    "    if viol.any():\n",
    "        sc = ax.scatter(pts[viol, 0], pts[viol, 1],\n",
    "                        c=ratio[viol], cmap='hot', s=4, vmin=1.0,\n",
    "                        vmax=min(ratio[viol].max(), 3.0), rasterized=True)\n",
    "        plt.colorbar(sc, ax=ax, label=r'$|C|/|A||\\bar{A}|$', shrink=0.8)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(r\"$m'$\")\n",
    "    ax.set_ylabel(r\"$\\theta'$\")\n",
    "    ax.set_title(f\"{title} - Violation: {viol.mean():.2%}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'violation_map_B_decay.pdf', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 13. Ensemble Training with Multiple Independent Datasets\n\nThis section generates multiple independent training datasets and runs ensemble training trials.\nEach trial uses a fresh dataset to capture statistical variations in the trained flows.\n\n### Workflow:\n1. **Generate datasets**: Create N sets of 2M events each for flavor, even, and odd\n2. **Train ensembles**: For each dataset, train all three flows jointly\n3. **Save results**: Each trial saved to separate directories"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 13a. Data Generation Functions\n# =============================================================================\nfrom DKpp import DKpp, DKppCorrelated, AmpSample\n\ndef generate_flavor_dataset(n_events, sdp_obj, seed=None):\n    \"\"\"\n    Generate D → Ksππ flavor-tagged events in SDP coordinates.\n    \n    Parameters\n    ----------\n    n_events : int\n        Number of events to generate\n    sdp_obj : SquareDalitzPlot2\n        SDP object for coordinate transformation\n    seed : int, optional\n        Random seed for reproducibility\n    \n    Returns\n    -------\n    ndarray, shape (n_events, 2)\n        Events in SDP coordinates (m', θ')\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    sampler = AmpSample(DKpp())\n    points_dp = sampler.generate(n_events, nbatch=50000)\n    points_sdp = dp_to_sdp(points_dp, sdp_obj, idx=(1,2,3))\n    return points_sdp\n\n\ndef generate_cp_even_dataset(n_events, sdp_obj, seed=None):\n    \"\"\"\n    Generate CP-even combination events in SDP coordinates.\n    \n    Parameters\n    ----------\n    n_events : int\n        Number of events to generate\n    sdp_obj : SquareDalitzPlot2\n        SDP object for coordinate transformation\n    seed : int, optional\n        Random seed for reproducibility\n    \n    Returns\n    -------\n    ndarray, shape (n_events, 2)\n        Events in SDP coordinates (m', θ')\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    sampler = AmpSample(DKppCorrelated(cp=+1))\n    points_dp = sampler.generate(n_events, nbatch=50000)\n    points_sdp = dp_to_sdp(points_dp, sdp_obj, idx=(1,2,3))\n    return points_sdp\n\n\ndef generate_cp_odd_dataset(n_events, sdp_obj, seed=None):\n    \"\"\"\n    Generate CP-odd combination events in SDP coordinates.\n    \n    Parameters\n    ----------\n    n_events : int\n        Number of events to generate\n    sdp_obj : SquareDalitzPlot2\n        SDP object for coordinate transformation\n    seed : int, optional\n        Random seed for reproducibility\n    \n    Returns\n    -------\n    ndarray, shape (n_events, 2)\n        Events in SDP coordinates (m', θ')\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    sampler = AmpSample(DKppCorrelated(cp=-1))\n    points_dp = sampler.generate(n_events, nbatch=50000)\n    points_sdp = dp_to_sdp(points_dp, sdp_obj, idx=(1,2,3))\n    return points_sdp\n\n\ndef generate_all_datasets(n_sets, n_events_per_set, sdp_obj, output_dir, base_seed=42):\n    \"\"\"\n    Generate multiple independent datasets for ensemble training.\n    \n    Creates directories:\n        output_dir/\n            flavor/\n                set_0.npy, set_1.npy, ...\n            even/\n                set_0.npy, set_1.npy, ...\n            odd/\n                set_0.npy, set_1.npy, ...\n    \n    Parameters\n    ----------\n    n_sets : int\n        Number of independent datasets to generate\n    n_events_per_set : int\n        Number of events in each dataset\n    sdp_obj : SquareDalitzPlot2\n        SDP object for coordinate transformation\n    output_dir : str or Path\n        Base directory to save datasets\n    base_seed : int\n        Base random seed (each set uses base_seed + set_index)\n    \"\"\"\n    output_dir = Path(output_dir)\n    \n    # Create subdirectories\n    flavor_dir = output_dir / \"flavor\"\n    even_dir = output_dir / \"even\"\n    odd_dir = output_dir / \"odd\"\n    \n    flavor_dir.mkdir(parents=True, exist_ok=True)\n    even_dir.mkdir(parents=True, exist_ok=True)\n    odd_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"Generating {n_sets} independent datasets with {n_events_per_set:,} events each\")\n    print(f\"Output directory: {output_dir}\")\n    print(\"=\" * 60)\n    \n    for i in range(n_sets):\n        seed = base_seed + i * 1000  # Different seed for each set\n        print(f\"\\n[Set {i+1}/{n_sets}] seed={seed}\")\n        \n        # Generate flavor\n        print(f\"  Generating flavor data...\", end=\" \", flush=True)\n        data_flavor = generate_flavor_dataset(n_events_per_set, sdp_obj, seed=seed)\n        np.save(flavor_dir / f\"set_{i}.npy\", data_flavor)\n        print(f\"saved {len(data_flavor):,} events\")\n        \n        # Generate CP-even\n        print(f\"  Generating CP-even data...\", end=\" \", flush=True)\n        data_even = generate_cp_even_dataset(n_events_per_set, sdp_obj, seed=seed+100)\n        np.save(even_dir / f\"set_{i}.npy\", data_even)\n        print(f\"saved {len(data_even):,} events\")\n        \n        # Generate CP-odd\n        print(f\"  Generating CP-odd data...\", end=\" \", flush=True)\n        data_odd = generate_cp_odd_dataset(n_events_per_set, sdp_obj, seed=seed+200)\n        np.save(odd_dir / f\"set_{i}.npy\", data_odd)\n        print(f\"saved {len(data_odd):,} events\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(f\"Done! Generated {n_sets} sets × 3 types = {n_sets * 3} files\")\n    print(f\"Total events: {n_sets * n_events_per_set * 3:,}\")\n    \n    return output_dir",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 13b. Ensemble Training Function\n# =============================================================================\n\ndef run_ensemble_training(\n    n_trials,\n    data_dir,\n    output_base_dir,\n    constraint_points,\n    constraint_points_swapped,\n    Gamma_plus,\n    Gamma_minus,\n    sdp_obj,\n    flow_config=None,\n    train_config=None,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n):\n    \"\"\"\n    Run ensemble training: train N independent flow triplets on N datasets.\n    \n    Parameters\n    ----------\n    n_trials : int\n        Number of training trials (should match number of dataset sets)\n    data_dir : str or Path\n        Directory containing flavor/, even/, odd/ subdirectories with set_*.npy files\n    output_base_dir : str or Path\n        Base directory for output. Creates:\n            output_base_dir/\n                ensemble_flavor/trial_0.pth, trial_1.pth, ...\n                ensemble_even/trial_0.pth, trial_1.pth, ...\n                ensemble_odd/trial_0.pth, trial_1.pth, ...\n                histories/trial_0.json, trial_1.json, ...\n    constraint_points : torch.Tensor\n        B-decay points for constraint evaluation\n    constraint_points_swapped : torch.Tensor\n        Swapped coordinates for constraint points\n    Gamma_plus, Gamma_minus : float\n        CP-even/odd decay widths\n    sdp_obj : SquareDalitzPlot2\n        SDP object for coordinate transformations\n    flow_config : dict, optional\n        Flow architecture config (num_flows, hidden_features, num_bins)\n    train_config : dict, optional\n        Training config (num_epochs, warmup_epochs, lr_*, lam, etc.)\n    device : str\n        Torch device\n    \n    Returns\n    -------\n    all_histories : list of dicts\n        Training history for each trial\n    \"\"\"\n    data_dir = Path(data_dir)\n    output_base_dir = Path(output_base_dir)\n    \n    # Default configs\n    if flow_config is None:\n        flow_config = {'num_flows': 8, 'hidden_features': 64, 'num_bins': 16}\n    if train_config is None:\n        train_config = {\n            'num_epochs': 100,\n            'warmup_epochs': 10,\n            'lr_odd': 1e-3,\n            'lr_even': 1e-3,\n            'lr_flavor': 1e-3,\n            'lam': 10.0\n        }\n    \n    # Create output directories\n    flavor_out = output_base_dir / \"ensemble_flavor\"\n    even_out = output_base_dir / \"ensemble_even\"\n    odd_out = output_base_dir / \"ensemble_odd\"\n    history_out = output_base_dir / \"histories\"\n    \n    flavor_out.mkdir(parents=True, exist_ok=True)\n    even_out.mkdir(parents=True, exist_ok=True)\n    odd_out.mkdir(parents=True, exist_ok=True)\n    history_out.mkdir(parents=True, exist_ok=True)\n    \n    # Save configs\n    config_save = {\n        'flow_config': flow_config,\n        'train_config': train_config,\n        'n_trials': n_trials,\n        'data_dir': str(data_dir),\n        'timestamp': datetime.now().isoformat()\n    }\n    with open(output_base_dir / \"ensemble_config.json\", 'w') as f:\n        json.dump(config_save, f, indent=2)\n    \n    all_histories = []\n    \n    print(\"=\" * 70)\n    print(f\"ENSEMBLE TRAINING: {n_trials} trials\")\n    print(f\"Data directory: {data_dir}\")\n    print(f\"Output directory: {output_base_dir}\")\n    print(f\"Flow config: {flow_config}\")\n    print(f\"Train config: {train_config}\")\n    print(\"=\" * 70)\n    \n    for trial in range(n_trials):\n        print(f\"\\n{'='*70}\")\n        print(f\"TRIAL {trial + 1}/{n_trials}\")\n        print(f\"{'='*70}\")\n        \n        # Load data for this trial\n        print(f\"\\nLoading dataset set_{trial}...\")\n        data_flavor = np.load(data_dir / \"flavor\" / f\"set_{trial}.npy\")\n        data_even = np.load(data_dir / \"even\" / f\"set_{trial}.npy\")\n        data_odd = np.load(data_dir / \"odd\" / f\"set_{trial}.npy\")\n        \n        print(f\"  Flavor: {len(data_flavor):,} events\")\n        print(f\"  Even:   {len(data_even):,} events\")\n        print(f\"  Odd:    {len(data_odd):,} events\")\n        \n        # Precompute swapped coordinates for odd data\n        print(\"Precomputing swapped coordinates for odd data...\")\n        data_odd_swapped = compute_swapped_sdp_coords(\n            data_odd, sdp_obj, idx=(1,2,3), pair_swap=(1,3,2)\n        )\n        \n        # Create dataset and loader\n        joint_dataset = JointDalitzDataset(data_flavor, data_even, data_odd, data_odd_swapped)\n        joint_loader = DataLoader(\n            joint_dataset,\n            batch_size=50000,\n            shuffle=True,\n            num_workers=0,\n            pin_memory=(device == \"cuda\")\n        )\n        \n        # Create fresh flows\n        print(\"Creating new flows...\")\n        flow_flavor = create_flow(**flow_config, device=device)\n        flow_even = create_flow(**flow_config, device=device)\n        flow_odd = create_flow(**flow_config, device=device)\n        \n        # Train\n        print(f\"\\nStarting training...\")\n        (flow_flavor_t, flow_even_t, flow_odd_t), history = train_joint_flows_with_B_constraint(\n            flow_flavor,\n            flow_even,\n            flow_odd,\n            joint_loader,\n            constraint_points,\n            constraint_points_swapped,\n            Gamma_plus=Gamma_plus,\n            Gamma_minus=Gamma_minus,\n            constraint_batch_size=50000,\n            device=device,\n            **train_config\n        )\n        \n        # Save models\n        torch.save(flow_flavor_t.state_dict(), flavor_out / f\"trial_{trial}.pth\")\n        torch.save(flow_even_t.state_dict(), even_out / f\"trial_{trial}.pth\")\n        torch.save(flow_odd_t.state_dict(), odd_out / f\"trial_{trial}.pth\")\n        \n        # Save history\n        with open(history_out / f\"trial_{trial}.json\", 'w') as f:\n            json.dump(history, f, indent=2)\n        \n        all_histories.append(history)\n        \n        # Print summary for this trial\n        print(f\"\\nTrial {trial + 1} complete:\")\n        print(f\"  Final NLL - Flavor: {history['nll_flavor'][-1]:.4f}\")\n        print(f\"  Final NLL - Even:   {history['nll_even'][-1]:.4f}\")\n        print(f\"  Final NLL - Odd:    {history['nll_odd'][-1]:.4f}\")\n        print(f\"  Final violation:    {history['violation_frac'][-1]:.4f}\")\n        \n        # Clear memory\n        del flow_flavor, flow_even, flow_odd\n        del flow_flavor_t, flow_even_t, flow_odd_t\n        del joint_dataset, joint_loader\n        del data_flavor, data_even, data_odd, data_odd_swapped\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ENSEMBLE TRAINING COMPLETE\")\n    print(\"=\" * 70)\n    print(f\"\\nModels saved to:\")\n    print(f\"  Flavor: {flavor_out}/\")\n    print(f\"  Even:   {even_out}/\")\n    print(f\"  Odd:    {odd_out}/\")\n    print(f\"  Histories: {history_out}/\")\n    \n    # Summary statistics\n    final_nlls_flavor = [h['nll_flavor'][-1] for h in all_histories]\n    final_nlls_even = [h['nll_even'][-1] for h in all_histories]\n    final_nlls_odd = [h['nll_odd'][-1] for h in all_histories]\n    final_viols = [h['violation_frac'][-1] for h in all_histories]\n    \n    print(f\"\\nSummary statistics across {n_trials} trials:\")\n    print(f\"  NLL Flavor: {np.mean(final_nlls_flavor):.4f} ± {np.std(final_nlls_flavor):.4f}\")\n    print(f\"  NLL Even:   {np.mean(final_nlls_even):.4f} ± {np.std(final_nlls_even):.4f}\")\n    print(f\"  NLL Odd:    {np.mean(final_nlls_odd):.4f} ± {np.std(final_nlls_odd):.4f}\")\n    print(f\"  Violation:  {np.mean(final_viols):.4f} ± {np.std(final_viols):.4f}\")\n    \n    return all_histories",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 13c. Generate Datasets (Run this ONCE to create all training data)\n# =============================================================================\n# Configuration\nN_SETS = 10           # Number of independent datasets\nN_EVENTS = 2_000_000  # Events per dataset (2M)\nDATA_DIR = \"ensemble_data_2e6\"\n\n# Generate all datasets\ngenerate_all_datasets(\n    n_sets=N_SETS,\n    n_events_per_set=N_EVENTS,\n    sdp_obj=sdp_obj,\n    output_dir=DATA_DIR,\n    base_seed=42\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 13d. Run Ensemble Training\n# =============================================================================\n# Make sure you have:\n#   1. Generated datasets (cell 13c above)\n#   2. Loaded B-decay constraint points (section 7b)\n#   3. Computed Gamma_plus/Gamma_minus (section 5)\n\n# Configuration\nN_TRIALS = 10\nDATA_DIR = \"ensemble_data_2e6\"\nOUTPUT_DIR = \"joint_ensemble_2e6\"\n\n# Flow architecture\nENSEMBLE_FLOW_CONFIG = {\n    'num_flows': 8,\n    'hidden_features': 64,\n    'num_bins': 16\n}\n\n# Training hyperparameters\nENSEMBLE_TRAIN_CONFIG = {\n    'num_epochs': 100,\n    'warmup_epochs': 10,\n    'lr_odd': 1e-3,\n    'lr_even': 1e-3,\n    'lr_flavor': 1e-3,\n    'lam': 10.0\n}\n\n# Run ensemble training\nall_histories = run_ensemble_training(\n    n_trials=N_TRIALS,\n    data_dir=DATA_DIR,\n    output_base_dir=OUTPUT_DIR,\n    constraint_points=constraint_points,\n    constraint_points_swapped=constraint_points_swapped,\n    Gamma_plus=gamma_p,\n    Gamma_minus=gamma_m,\n    sdp_obj=sdp_obj,\n    flow_config=ENSEMBLE_FLOW_CONFIG,\n    train_config=ENSEMBLE_TRAIN_CONFIG,\n    device=device\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 13e. Plot Ensemble Results\n# =============================================================================\n\ndef plot_ensemble_histories(all_histories, output_dir=None):\n    \"\"\"Plot training curves for all ensemble trials.\"\"\"\n    n_trials = len(all_histories)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n    colors = plt.cm.viridis(np.linspace(0, 1, n_trials))\n    \n    # NLL Flavor\n    ax = axes[0, 0]\n    for i, h in enumerate(all_histories):\n        ax.plot(h['nll_flavor'], color=colors[i], alpha=0.7, linewidth=1)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('NLL')\n    ax.set_title('NLL Flavor (all trials)')\n    ax.grid(True, alpha=0.3)\n    \n    # NLL Even\n    ax = axes[0, 1]\n    for i, h in enumerate(all_histories):\n        ax.plot(h['nll_even'], color=colors[i], alpha=0.7, linewidth=1)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('NLL')\n    ax.set_title('NLL Even (all trials)')\n    ax.grid(True, alpha=0.3)\n    \n    # NLL Odd\n    ax = axes[0, 2]\n    for i, h in enumerate(all_histories):\n        ax.plot(h['nll_odd'], color=colors[i], alpha=0.7, linewidth=1)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('NLL')\n    ax.set_title('NLL Odd (all trials)')\n    ax.grid(True, alpha=0.3)\n    \n    # Violation fraction\n    ax = axes[1, 0]\n    for i, h in enumerate(all_histories):\n        ax.plot(h['violation_frac'], color=colors[i], alpha=0.7, linewidth=1)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Fraction')\n    ax.set_title('Violation Fraction (all trials)')\n    ax.grid(True, alpha=0.3)\n    \n    # Penalty\n    ax = axes[1, 1]\n    for i, h in enumerate(all_histories):\n        ax.plot(h['penalty_loss'], color=colors[i], alpha=0.7, linewidth=1)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Penalty')\n    ax.set_title('Constraint Penalty (all trials)')\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n    \n    # Final NLL distribution\n    ax = axes[1, 2]\n    final_flavor = [h['nll_flavor'][-1] for h in all_histories]\n    final_even = [h['nll_even'][-1] for h in all_histories]\n    final_odd = [h['nll_odd'][-1] for h in all_histories]\n    \n    positions = [0, 1, 2]\n    ax.boxplot([final_flavor, final_even, final_odd], positions=positions)\n    ax.set_xticks(positions)\n    ax.set_xticklabels(['Flavor', 'Even', 'Odd'])\n    ax.set_ylabel('Final NLL')\n    ax.set_title('Final NLL Distribution')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    if output_dir:\n        plt.savefig(Path(output_dir) / 'ensemble_training_summary.pdf', dpi=150, bbox_inches='tight')\n    \n    plt.show()\n    \n    # Print statistics\n    print(\"\\n\" + \"=\" * 50)\n    print(\"ENSEMBLE STATISTICS\")\n    print(\"=\" * 50)\n    print(f\"\\nFinal NLL (mean ± std):\")\n    print(f\"  Flavor: {np.mean(final_flavor):.4f} ± {np.std(final_flavor):.4f}\")\n    print(f\"  Even:   {np.mean(final_even):.4f} ± {np.std(final_even):.4f}\")\n    print(f\"  Odd:    {np.mean(final_odd):.4f} ± {np.std(final_odd):.4f}\")\n    \n    final_viol = [h['violation_frac'][-1] for h in all_histories]\n    print(f\"\\nFinal violation fraction:\")\n    print(f\"  Mean: {np.mean(final_viol):.4f} ± {np.std(final_viol):.4f}\")\n    print(f\"  Range: [{min(final_viol):.4f}, {max(final_viol):.4f}]\")\n\n# Plot results\nplot_ensemble_histories(all_histories, output_dir=OUTPUT_DIR)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 14. Loading Ensemble Models for Inference\n\nAfter training, load the ensemble models for γ extraction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 14. Load Ensemble Models\n# =============================================================================\n\ndef load_ensemble_flows(ensemble_dir, flow_config, n_trials=None, device='cpu'):\n    \"\"\"\n    Load all trained flows from an ensemble directory.\n    \n    Parameters\n    ----------\n    ensemble_dir : str or Path\n        Directory containing ensemble_flavor/, ensemble_even/, ensemble_odd/\n    flow_config : dict\n        Flow architecture config used during training\n    n_trials : int, optional\n        Number of trials to load (default: auto-detect)\n    device : str\n        Torch device\n    \n    Returns\n    -------\n    flows_flavor : list of Flow\n    flows_even : list of Flow\n    flows_odd : list of Flow\n    \"\"\"\n    ensemble_dir = Path(ensemble_dir)\n    \n    # Auto-detect number of trials\n    if n_trials is None:\n        flavor_files = list((ensemble_dir / \"ensemble_flavor\").glob(\"trial_*.pth\"))\n        n_trials = len(flavor_files)\n    \n    print(f\"Loading {n_trials} ensemble models from {ensemble_dir}\")\n    \n    flows_flavor = []\n    flows_even = []\n    flows_odd = []\n    \n    for i in range(n_trials):\n        # Create fresh flows\n        f_flavor = create_flow(**flow_config, device=device)\n        f_even = create_flow(**flow_config, device=device)\n        f_odd = create_flow(**flow_config, device=device)\n        \n        # Load weights\n        f_flavor.load_state_dict(torch.load(\n            ensemble_dir / \"ensemble_flavor\" / f\"trial_{i}.pth\", \n            map_location=device\n        ))\n        f_even.load_state_dict(torch.load(\n            ensemble_dir / \"ensemble_even\" / f\"trial_{i}.pth\",\n            map_location=device\n        ))\n        f_odd.load_state_dict(torch.load(\n            ensemble_dir / \"ensemble_odd\" / f\"trial_{i}.pth\",\n            map_location=device\n        ))\n        \n        # Set to eval mode\n        f_flavor.eval()\n        f_even.eval()\n        f_odd.eval()\n        \n        flows_flavor.append(f_flavor)\n        flows_even.append(f_even)\n        flows_odd.append(f_odd)\n        \n        print(f\"  Loaded trial {i}\")\n    \n    print(f\"Done! Loaded {n_trials} × 3 = {n_trials * 3} models\")\n    \n    return flows_flavor, flows_even, flows_odd\n\n\n# Example usage (uncomment to run):\n# flows_flavor, flows_even, flows_odd = load_ensemble_flows(\n#     ensemble_dir=\"joint_ensemble_2e6\",\n#     flow_config=ENSEMBLE_FLOW_CONFIG,\n#     device=device\n# )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}